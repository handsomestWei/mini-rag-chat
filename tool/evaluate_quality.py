"""
å‘é‡åº“æ•°æ®è´¨é‡è¯„ä¼°å·¥å…·
ç”¨äºè¯„ä¼°å‘é‡åŒ–è´¨é‡å’Œä¸ºç›¸ä¼¼åº¦é˜ˆå€¼è®¾å®šæä¾›å‚è€ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
    python tool/evaluate_quality.py [é€‰é¡¹]

é€‰é¡¹ï¼š
    --top-k N          è¿”å›å‰Nä¸ªç»“æœï¼ˆé»˜è®¤ï¼š10ï¼‰
    --output FILE      ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡ä»¶
    --verbose          æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    --help             æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹ï¼š
    # ä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹è¯„ä¼°
    python tool/evaluate_quality.py

    # è¿”å›å‰20ä¸ªç»“æœ
    python tool/evaluate_quality.py --top-k 20

    # ä¿å­˜æŠ¥å‘Š
    python tool/evaluate_quality.py --output evaluation_report.txt

    # è¯¦ç»†æ¨¡å¼
    python tool/evaluate_quality.py --verbose

è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼š
    ç›´æ¥ç¼–è¾‘æœ¬æ–‡ä»¶é¡¶éƒ¨çš„ TEST_CASES åˆ—è¡¨å³å¯
"""

import sys
import io
import os
import argparse
from datetime import datetime

# ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import warnings
warnings.filterwarnings('ignore')


# ========== æµ‹è¯•ç”¨ä¾‹é…ç½® ==========
# å¯ä»¥ç›´æ¥åœ¨è¿™é‡Œä¿®æ”¹æµ‹è¯•é—®é¢˜å’ŒæœŸæœ›å…³é”®è¯

TEST_CASES = [
    # ç–¾ç—…ç›¸å…³
    {
        "question": "ç‹—ç‹—çƒ§çƒ«ä¼¤æ€ä¹ˆåŠ",
        "keywords": ["çƒ§çƒ«ä¼¤", "æŠ¤ç†", "ç‚¹æ»´", "æŠ—ç”Ÿç´ "]
    },

    # æ—¥å¸¸æŠ¤ç†
    {
        "question": "å…»ç‹—å‰çš„é¡»çŸ¥",
        "keywords": ["å…»ç‹—", "å‡†å¤‡", "æ³¨æ„äº‹é¡¹"]
    }
]

# å¦‚æœä½ æƒ³æ·»åŠ æ–°çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒæŒ‰ä»¥ä¸‹æ ¼å¼æ·»åŠ ï¼š
# {
#     "question": "ä½ çš„é—®é¢˜",
#     "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]
# },


class VectorStoreEvaluator:
    """å‘é‡åº“è´¨é‡è¯„ä¼°å™¨"""

    def __init__(self, config_obj, top_k=10):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            config_obj: é…ç½®å¯¹è±¡
            top_k: è¿”å›å‰Nä¸ªç»“æœ
        """
        self.config = config_obj
        self.top_k = top_k
        self.embeddings = None
        self.vector_store = None

        # åŠ è½½æ¨¡å‹å’Œå‘é‡åº“
        self._load_models()

    def _load_models(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹å’Œå‘é‡åº“"""
        print("ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.EMBEDDING_MODEL_PATH,
            model_kwargs={"device": "cpu"},
            encode_kwargs={
                "batch_size": self.config.EMBEDDING_BATCH_SIZE,
                "normalize_embeddings": True
            }
        )
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {self.config.EMBEDDING_MODEL_PATH}")

        print("\nğŸ“š åŠ è½½å‘é‡åº“...")
        if not os.path.exists(self.config.VECTOR_STORE_PATH):
            print(f"âŒ å‘é‡åº“ä¸å­˜åœ¨: {self.config.VECTOR_STORE_PATH}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œåº”ç”¨åˆ›å»ºå‘é‡åº“: python app_new.py")
            sys.exit(1)

        self.vector_store = FAISS.load_local(
            self.config.VECTOR_STORE_PATH,
            self.embeddings,
            allow_dangerous_deserialization=True
        )

        # è·å–å‘é‡åº“ç»Ÿè®¡
        total_docs = len(self.vector_store.docstore._dict)
        print(f"âœ… å‘é‡åº“åŠ è½½å®Œæˆ: {total_docs} ä¸ªæ–‡æ¡£å—")

    def evaluate_query(self, question, expected_keywords=None, verbose=False):
        """
        è¯„ä¼°å•ä¸ªæŸ¥è¯¢çš„è´¨é‡

        Args:
            question: æµ‹è¯•é—®é¢˜
            expected_keywords: æœŸæœ›çš„å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºéªŒè¯ï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            evaluation: è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\n" + "=" * 80)
        print(f"ğŸ” é—®é¢˜: {question}")
        if expected_keywords:
            print(f"ğŸ“Œ æœŸæœ›å…³é”®è¯: {', '.join(expected_keywords)}")
        print("=" * 80)

        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        docs_with_scores = self.vector_store.similarity_search_with_score(
            question,
            k=self.top_k
        )

        # åˆ†æç»“æœ
        results = []
        keyword_hits = {kw: [] for kw in (expected_keywords or [])}

        for i, (doc, distance) in enumerate(docs_with_scores, 1):
            # FAISS L2è·ç¦»è½¬ç›¸ä¼¼åº¦: similarity = 1 / (1 + distance)
            similarity = 1 / (1 + distance)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸæœ›å…³é”®è¯
            hits = []
            if expected_keywords:
                for keyword in expected_keywords:
                    if keyword in doc.page_content:
                        hits.append(keyword)
                        keyword_hits[keyword].append(i)

            result = {
                'rank': i,
                'distance': distance,
                'similarity': similarity,
                'content': doc.page_content,
                'source': doc.metadata.get('source', 'Unknown'),
                'keyword_hits': hits
            }
            results.append(result)

            # æ˜¾ç¤ºç»“æœ
            content_preview = doc.page_content[:100].replace('\n', ' ')

            print(f"\nã€ç¬¬ {i} åã€‘")
            print(f"  ğŸ¯ L2è·ç¦»: {distance:.4f}")
            print(f"  ğŸ“ˆ ç›¸ä¼¼åº¦: {similarity:.4f}")

            if hits:
                print(f"  âœ… å‘½ä¸­å…³é”®è¯: {', '.join(hits)}")
            elif expected_keywords:
                print(f"  âŒ æœªå‘½ä¸­å…³é”®è¯")

            print(f"  ğŸ“„ å†…å®¹: {content_preview}...")

            if verbose:
                print(f"  ğŸ“‹ æ¥æº: {result['source']}")
                if len(doc.page_content) > 100:
                    print(f"  ğŸ“ å®Œæ•´å†…å®¹:\n{doc.page_content[:300]}...")

        # åˆ†æå’Œå»ºè®®
        evaluation = self._analyze_results(
            question,
            results,
            keyword_hits,
            expected_keywords
        )

        return evaluation

    def _analyze_results(self, question, results, keyword_hits, expected_keywords):
        """åˆ†æè¯„ä¼°ç»“æœå¹¶æä¾›å»ºè®®"""
        print("\n" + "=" * 80)
        print("ğŸ“Š è¯„ä¼°åˆ†æ")
        print("=" * 80)

        # ç›¸ä¼¼åº¦ç»Ÿè®¡
        similarities = [r['similarity'] for r in results]
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0
        max_similarity = max(similarities) if similarities else 0

        print(f"\nğŸ“ˆ ç›¸ä¼¼åº¦ç»Ÿè®¡:")
        print(f"  - æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.4f}")
        print(f"  - å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
        print(f"  - Top-3å¹³å‡: {sum(similarities[:3])/3:.4f}")
        print(f"  - Top-5å¹³å‡: {sum(similarities[:5])/5:.4f}")

        # å…³é”®è¯å‘½ä¸­åˆ†æ
        if expected_keywords:
            print(f"\nğŸ¯ å…³é”®è¯å‘½ä¸­åˆ†æ:")
            for keyword, ranks in keyword_hits.items():
                if ranks:
                    best_rank = min(ranks)
                    print(f"  âœ… '{keyword}': æœ€ä½³æ’åç¬¬ {best_rank} ä½ "
                          f"(å…± {len(ranks)} ä¸ªå—åŒ…å«)")
                else:
                    print(f"  âŒ '{keyword}': æœªåœ¨å‰{self.top_k}åä¸­æ‰¾åˆ°")

        # è´¨é‡è¯„çº§
        print(f"\nâ­ è´¨é‡è¯„çº§:")
        quality_score, quality_grade = self._calculate_quality_score(
            max_similarity,
            avg_similarity,
            keyword_hits,
            expected_keywords
        )
        print(f"  - ç»¼åˆå¾—åˆ†: {quality_score:.2f}/100")
        print(f"  - è´¨é‡ç­‰çº§: {quality_grade}")

        # é˜ˆå€¼å»ºè®®
        print(f"\nğŸ’¡ é˜ˆå€¼å»ºè®®:")
        threshold_suggestions = self._suggest_thresholds(similarities)
        for mode, threshold in threshold_suggestions.items():
            print(f"  - {mode}: {threshold:.2f}")

        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ”§ æ”¹è¿›å»ºè®®:")
        suggestions = self._generate_suggestions(
            max_similarity,
            avg_similarity,
            keyword_hits,
            expected_keywords
        )
        for suggestion in suggestions:
            print(f"  â€¢ {suggestion}")

        print("=" * 80)

        return {
            'question': question,
            'results': results,
            'max_similarity': max_similarity,
            'avg_similarity': avg_similarity,
            'quality_score': quality_score,
            'quality_grade': quality_grade,
            'threshold_suggestions': threshold_suggestions,
            'keyword_hits': keyword_hits
        }

    def _calculate_quality_score(self, max_sim, avg_sim, keyword_hits, expected_keywords):
        """è®¡ç®—è´¨é‡å¾—åˆ†ï¼ˆ0-100ï¼‰"""
        score = 0

        # æœ€é«˜ç›¸ä¼¼åº¦ï¼ˆ40åˆ†ï¼‰
        if max_sim >= 0.7:
            score += 40
        elif max_sim >= 0.6:
            score += 30
        elif max_sim >= 0.5:
            score += 20
        elif max_sim >= 0.4:
            score += 10

        # å¹³å‡ç›¸ä¼¼åº¦ï¼ˆ30åˆ†ï¼‰
        if avg_sim >= 0.6:
            score += 30
        elif avg_sim >= 0.5:
            score += 20
        elif avg_sim >= 0.4:
            score += 10

        # å…³é”®è¯å‘½ä¸­ï¼ˆ30åˆ†ï¼‰
        if expected_keywords:
            hit_count = sum(1 for ranks in keyword_hits.values() if ranks)
            hit_rate = hit_count / len(expected_keywords)

            # å‘½ä¸­ç‡å¾—åˆ†ï¼ˆ15åˆ†ï¼‰
            score += hit_rate * 15

            # æ’åå¾—åˆ†ï¼ˆ15åˆ†ï¼‰
            if hit_count > 0:
                avg_rank = sum(min(ranks) for ranks in keyword_hits.values() if ranks) / hit_count
                if avg_rank <= 3:
                    score += 15
                elif avg_rank <= 5:
                    score += 10
                elif avg_rank <= 10:
                    score += 5

        # è´¨é‡ç­‰çº§
        if score >= 80:
            grade = "ä¼˜ç§€ â­â­â­â­â­"
        elif score >= 60:
            grade = "è‰¯å¥½ â­â­â­â­"
        elif score >= 40:
            grade = "ä¸€èˆ¬ â­â­â­"
        elif score >= 20:
            grade = "è¾ƒå·® â­â­"
        else:
            grade = "å¾ˆå·® â­"

        return score, grade

    def _suggest_thresholds(self, similarities):
        """æ ¹æ®ç›¸ä¼¼åº¦åˆ†å¸ƒå»ºè®®é˜ˆå€¼"""
        if not similarities:
            return {}

        top3_avg = sum(similarities[:3]) / 3
        top5_avg = sum(similarities[:5]) / 5
        max_sim = max(similarities)

        return {
            "ä¸¥æ ¼æ¨¡å¼ï¼ˆé«˜ç²¾å‡†ï¼‰": max(0.3, top3_avg - 0.05),
            "å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰": max(0.25, top5_avg - 0.08),
            "å®½æ¾æ¨¡å¼ï¼ˆé«˜å¬å›ï¼‰": max(0.2, top5_avg - 0.15)
        }

    def _generate_suggestions(self, max_sim, avg_sim, keyword_hits, expected_keywords):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        # ç›¸ä¼¼åº¦åˆ†æ
        if max_sim < 0.5:
            suggestions.append("âš ï¸ æœ€é«˜ç›¸ä¼¼åº¦è¿‡ä½ - å»ºè®®é‡å»ºå‘é‡åº“æˆ–ä¼˜åŒ–æ–‡æ¡£åˆ†å—")

        if avg_sim < 0.4:
            suggestions.append("âš ï¸ å¹³å‡ç›¸ä¼¼åº¦è¿‡ä½ - æ£€ç´¢è´¨é‡å·®ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®è´¨é‡")

        # å…³é”®è¯åˆ†æ
        if expected_keywords:
            not_found = [kw for kw, ranks in keyword_hits.items() if not ranks]
            if not_found:
                suggestions.append(f"âŒ å…³é”®è¯æœªå‘½ä¸­: {', '.join(not_found)} - æ£€æŸ¥æ–‡æ¡£æ˜¯å¦åŒ…å«ç›¸å…³å†…å®¹")

            low_rank = [kw for kw, ranks in keyword_hits.items() if ranks and min(ranks) > 10]
            if low_rank:
                suggestions.append(f"âš ï¸ å…³é”®è¯æ’åé å: {', '.join(low_rank)} - è€ƒè™‘å‡å°CHUNK_SIZE")

        # é€šç”¨å»ºè®®
        if max_sim >= 0.6 and avg_sim >= 0.5:
            suggestions.append("âœ… å‘é‡åº“è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ä½¿ç”¨å½“å‰é…ç½®")

        if not suggestions:
            suggestions.append("âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œæ— éœ€ä¼˜åŒ–")

        return suggestions

    def evaluate_batch(self, test_cases, verbose=False):
        """
        æ‰¹é‡è¯„ä¼°å¤šä¸ªæµ‹è¯•é—®é¢˜

        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ [{"question": "...", "keywords": [...]}, ...]
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            evaluations: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        print("\n" + "=" * 80)
        print("ğŸ”¬ å‘é‡åº“æ•°æ®è´¨é‡è¯„ä¼°")
        print("=" * 80)
        print(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}")
        print(f"è¿”å›ç»“æœæ•°: Top-{self.top_k}")
        print(f"å‘é‡åº“è·¯å¾„: {self.config.VECTOR_STORE_PATH}")
        print(f"åµŒå…¥æ¨¡å‹: {self.config.EMBEDDING_MODEL_PATH}")
        print("=" * 80)

        evaluations = []

        for i, test_case in enumerate(test_cases, 1):
            question = test_case.get('question', '')
            keywords = test_case.get('keywords', [])

            print(f"\n{'='*20} æµ‹è¯•ç”¨ä¾‹ {i}/{len(test_cases)} {'='*20}")

            evaluation = self.evaluate_query(question, keywords, verbose)
            evaluations.append(evaluation)

        # ç”Ÿæˆæ€»ä½“è¯„ä¼°
        self._generate_summary(evaluations)

        return evaluations

    def _generate_summary(self, evaluations):
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ æ€»ä½“è¯„ä¼°æ‘˜è¦")
        print("=" * 80)

        # è®¡ç®—å¹³å‡åˆ†
        avg_quality_score = sum(e['quality_score'] for e in evaluations) / len(evaluations)
        avg_max_sim = sum(e['max_similarity'] for e in evaluations) / len(evaluations)
        avg_avg_sim = sum(e['avg_similarity'] for e in evaluations) / len(evaluations)

        print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
        print(f"  - æµ‹è¯•ç”¨ä¾‹æ•°: {len(evaluations)}")
        print(f"  - å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality_score:.2f}/100")
        print(f"  - å¹³å‡æœ€é«˜ç›¸ä¼¼åº¦: {avg_max_sim:.4f}")
        print(f"  - å¹³å‡å¹³å‡ç›¸ä¼¼åº¦: {avg_avg_sim:.4f}")

        # è´¨é‡åˆ†çº§ç»Ÿè®¡
        grades = [e['quality_grade'] for e in evaluations]
        print(f"\nâ­ è´¨é‡åˆ†å¸ƒ:")
        grade_counts = {}
        for grade in grades:
            grade_key = grade.split()[0]
            grade_counts[grade_key] = grade_counts.get(grade_key, 0) + 1

        for grade, count in sorted(grade_counts.items(), reverse=True):
            print(f"  - {grade}: {count} ä¸ªç”¨ä¾‹")

        # æ¨èé˜ˆå€¼
        all_top3_sims = []
        all_top5_sims = []
        for e in evaluations:
            sims = [r['similarity'] for r in e['results']]
            all_top3_sims.append(sum(sims[:3])/3)
            all_top5_sims.append(sum(sims[:5])/5)

        global_top3_avg = sum(all_top3_sims) / len(all_top3_sims)
        global_top5_avg = sum(all_top5_sims) / len(all_top5_sims)

        print(f"\nğŸ’¡ å…¨å±€é˜ˆå€¼å»ºè®®:")
        print(f"  - ä¸¥æ ¼æ¨¡å¼: {max(0.3, global_top3_avg - 0.05):.2f} (é€‚åˆé«˜è´¨é‡å›ç­”)")
        print(f"  - å¹³è¡¡æ¨¡å¼: {max(0.25, global_top5_avg - 0.08):.2f} (æ¨è)")
        print(f"  - å®½æ¾æ¨¡å¼: {max(0.2, global_top5_avg - 0.15):.2f} (é€‚åˆé«˜å¬å›ç‡)")

        # æ€»ä½“å»ºè®®
        print(f"\nğŸ”§ æ€»ä½“å»ºè®®:")
        if avg_quality_score >= 70:
            print("  âœ… å‘é‡åº“è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨")
            print(f"  ğŸ’¡ å»ºè®®é…ç½®: SEARCH_TYPE='similarity', RETRIEVER_K=4")
        elif avg_quality_score >= 50:
            print("  âš ï¸ å‘é‡åº“è´¨é‡è‰¯å¥½ï¼Œä½†æœ‰ä¼˜åŒ–ç©ºé—´")
            print("  ğŸ’¡ å»ºè®®: æ¸…ç†æ–‡æœ¬æ•°æ®åé‡å»ºå‘é‡åº“")
        else:
            print("  âŒ å‘é‡åº“è´¨é‡è¾ƒå·®ï¼Œå¼ºçƒˆå»ºè®®ä¼˜åŒ–")
            print("  ğŸ’¡ å»ºè®®:")
            print("     1. ä½¿ç”¨ tool/clean_text.py æ¸…æ´—æ–‡æœ¬")
            print("     2. è°ƒæ•´ CHUNK_SIZE å‚æ•°ï¼ˆå°è¯•400-600ï¼‰")
            print("     3. é‡å»ºå‘é‡åº“")

        print("=" * 80)




def save_report(evaluations, output_file):
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    print(f"\nğŸ’¾ ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°: {output_file}")

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("å‘é‡åº“æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(evaluations)}\n")
        f.write("\n")

        # è¯¦ç»†ç»“æœ
        for i, evaluation in enumerate(evaluations, 1):
            f.write(f"\n{'='*80}\n")
            f.write(f"æµ‹è¯•ç”¨ä¾‹ {i}: {evaluation['question']}\n")
            f.write(f"{'='*80}\n")
            f.write(f"è´¨é‡å¾—åˆ†: {evaluation['quality_score']:.2f}/100\n")
            f.write(f"è´¨é‡ç­‰çº§: {evaluation['quality_grade']}\n")
            f.write(f"æœ€é«˜ç›¸ä¼¼åº¦: {evaluation['max_similarity']:.4f}\n")
            f.write(f"å¹³å‡ç›¸ä¼¼åº¦: {evaluation['avg_similarity']:.4f}\n")
            f.write("\n")

            # Top-5ç»“æœ
            f.write("Top-5 æ£€ç´¢ç»“æœ:\n")
            f.write("-" * 80 + "\n")
            for result in evaluation['results'][:5]:
                f.write(f"ç¬¬{result['rank']}å | ç›¸ä¼¼åº¦: {result['similarity']:.4f} | "
                       f"æ¥æº: {os.path.basename(result['source'])}\n")
                f.write(f"å†…å®¹: {result['content'][:150]}...\n")
                f.write("-" * 80 + "\n")

        # æ€»ä½“æ‘˜è¦
        avg_quality = sum(e['quality_score'] for e in evaluations) / len(evaluations)
        f.write(f"\n{'='*80}\n")
        f.write("æ€»ä½“è¯„ä¼°\n")
        f.write(f"{'='*80}\n")
        f.write(f"å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.2f}/100\n")

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å‘é‡åº“æ•°æ®è´¨é‡è¯„ä¼°å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument('--top-k', type=int, default=10,
                        help='è¿”å›å‰Nä¸ªç»“æœï¼ˆé»˜è®¤ï¼š10ï¼‰')
    parser.add_argument('--output', metavar='FILE',
                        help='ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    args = parser.parse_args()

    # ä½¿ç”¨å†…ç½®æµ‹è¯•ç”¨ä¾‹
    print(f"ğŸ“‹ ä½¿ç”¨ {len(TEST_CASES)} ä¸ªå†…ç½®æµ‹è¯•ç”¨ä¾‹")
    print("ğŸ’¡ è¦ä¿®æ”¹æµ‹è¯•ç”¨ä¾‹ï¼Œè¯·ç¼–è¾‘æœ¬æ–‡ä»¶é¡¶éƒ¨çš„ TEST_CASES åˆ—è¡¨\n")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = VectorStoreEvaluator(config, top_k=args.top_k)

    # æ‰§è¡Œè¯„ä¼°
    evaluations = evaluator.evaluate_batch(TEST_CASES, verbose=args.verbose)

    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        save_report(evaluations, args.output)

    print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()

