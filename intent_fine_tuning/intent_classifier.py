#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å›¾åˆ†ç±»å™¨ - åŸºäºŽm3e-smallçš„è½»é‡çº§æ„å›¾è¯†åˆ«
"""

import os
import json
import logging
import numpy as np
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import joblib
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class IntentClassifier:
    """åŸºäºŽm3e-smallçš„æ„å›¾åˆ†ç±»å™¨"""

    def __init__(self, model_path: str = "../model/moka-ai/m3e-small"):
        """
        åˆå§‹åŒ–æ„å›¾åˆ†ç±»å™¨

        Args:
            model_path: m3e-smallæ¨¡åž‹è·¯å¾„
        """
        self.model_path = model_path
        self.embedding_model = None
        self.classifier = None
        self.label_encoder = None

        # æ„å›¾ç±»åˆ«å®šä¹‰
        self.intent_classes = [
            "greeting",      # é—®å€™
            "knowledge",     # çŸ¥è¯†é—®ç­”
            "politeness",    # ç¤¼è²Œç”¨è¯­
            "system",        # ç³»ç»Ÿäº¤äº’
            "chitchat",      # é—²èŠï¼ˆæ—¶é—´ã€å¤©æ°”ã€ç¬‘è¯ç­‰ï¼‰
            "unknown"        # æœªçŸ¥/å…¶ä»–
        ]

        # æ„å›¾é…ç½®
        self.intent_config = {
            "greeting": {
                "skip_rag": True,
                "response": "ä½ å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
            },
            "politeness": {
                "skip_rag": True,
                "response": "ä¸å®¢æ°”ï¼å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶æé—®ã€‚"
            },
            "system": {
                "skip_rag": True,
                "response": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›žç­”å…³äºŽç³»ç»ŸæŠ€æœ¯ç»†èŠ‚çš„é—®é¢˜ã€‚è¯·è¯¢é—®çŸ¥è¯†åº“ç›¸å…³çš„å†…å®¹ã€‚"
            },
            "chitchat": {
                "skip_rag": True,
                "response": "æŠ±æ­‰ï¼Œæˆ‘æ˜¯ä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œæš‚æ—¶æ— æ³•å›žç­”æ—¶é—´ã€å¤©æ°”ç­‰é—²èŠé—®é¢˜ã€‚è¯·è¯¢é—®çŸ¥è¯†åº“ç›¸å…³çš„å†…å®¹ã€‚"
            },
            "knowledge": {
                "skip_rag": False,
                "response": None  # æ­£å¸¸RAGæµç¨‹
            },
            "unknown": {
                "skip_rag": True,
                "response": "æŠ±æ­‰ï¼Œæˆ‘è¿˜ä¸èƒ½ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·è¡¥å……å®Œå–„æ‚¨çš„æé—®ã€‚"
            }
        }

        self._load_embedding_model()

    def _load_embedding_model(self):
        """åŠ è½½embeddingæ¨¡åž‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½embeddingæ¨¡åž‹: {self.model_path}")
            self.embedding_model = SentenceTransformer(self.model_path)
            logger.info("embeddingæ¨¡åž‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"åŠ è½½embeddingæ¨¡åž‹å¤±è´¥: {e}")
            raise

    def load_training_data(self,
                          general_file: str = "training_data_general.json",
                          domain_file: str = "training_data_domain.json") -> List[Tuple[str, str]]:
        """
        ä»Žå¤–éƒ¨æ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆæ”¯æŒé€šç”¨æ•°æ®å’Œé¢†åŸŸæ•°æ®åˆ†ç¦»ï¼‰

        Args:
            general_file: é€šç”¨è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé—®å€™ã€ç¤¼è²Œã€ç³»ç»Ÿã€é—²èŠç­‰ï¼‰
            domain_file: é¢†åŸŸè®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆçŸ¥è¯†é—®ç­”ï¼‰

        Returns:
            List[Tuple[str, str]]: (æ–‡æœ¬, æ„å›¾æ ‡ç­¾) çš„åˆ—è¡¨
        """
        training_data = []

        # åŠ è½½é€šç”¨æ•°æ®
        try:
            with open(general_file, 'r', encoding='utf-8') as f:
                general_data = json.load(f)

            for intent, texts in general_data.items():
                if intent in self.intent_classes:
                    for text in texts:
                        training_data.append((text, intent))

            logger.info(f"ä»Ž {general_file} åŠ è½½äº† {len(training_data)} æ¡é€šç”¨æ•°æ®")
        except FileNotFoundError:
            logger.warning(f"é€šç”¨æ•°æ®æ–‡ä»¶ {general_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        except Exception as e:
            logger.error(f"åŠ è½½é€šç”¨æ•°æ®å¤±è´¥: {e}")
            raise

        # åŠ è½½é¢†åŸŸæ•°æ®
        try:
            with open(domain_file, 'r', encoding='utf-8') as f:
                domain_data = json.load(f)

            domain_count = 0
            for intent, texts in domain_data.items():
                if intent in self.intent_classes:
                    for text in texts:
                        training_data.append((text, intent))
                        domain_count += 1

            logger.info(f"ä»Ž {domain_file} åŠ è½½äº† {domain_count} æ¡é¢†åŸŸæ•°æ®")
        except FileNotFoundError:
            logger.warning(f"é¢†åŸŸæ•°æ®æ–‡ä»¶ {domain_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        except Exception as e:
            logger.error(f"åŠ è½½é¢†åŸŸæ•°æ®å¤±è´¥: {e}")
            raise

        if len(training_data) == 0:
            logger.error("æœªåŠ è½½åˆ°ä»»ä½•è®­ç»ƒæ•°æ®")
            raise ValueError("æœªåŠ è½½åˆ°ä»»ä½•è®­ç»ƒæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")

        logger.info(f"æ€»å…±åŠ è½½äº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®")
        return training_data


    def prepare_training_data(self,
                             general_file: str = "training_data_general.json",
                             domain_file: str = "training_data_domain.json") -> List[Tuple[str, str]]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Args:
            general_file: é€šç”¨è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            domain_file: é¢†åŸŸè®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            List[Tuple[str, str]]: (æ–‡æœ¬, æ„å›¾æ ‡ç­¾) çš„åˆ—è¡¨
        """
        return self.load_training_data(general_file, domain_file)

    def extract_features(self, texts: List[str]) -> np.ndarray:
        """
        æå–æ–‡æœ¬ç‰¹å¾ï¼ˆembeddingï¼‰

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            np.ndarray: ç‰¹å¾çŸ©é˜µ
        """
        logger.info(f"æ­£åœ¨æå– {len(texts)} ä¸ªæ–‡æœ¬çš„ç‰¹å¾...")

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )

        logger.info(f"ç‰¹å¾æå–å®Œæˆï¼Œç»´åº¦: {embeddings.shape}")
        return embeddings

    def train_classifier(self, training_data: List[Tuple[str, str]], test_size: float = 0.2):
        """
        è®­ç»ƒåˆ†ç±»å™¨

        Args:
            training_data: è®­ç»ƒæ•°æ®
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
        """
        logger.info("å¼€å§‹è®­ç»ƒæ„å›¾åˆ†ç±»å™¨...")

        # åˆ†ç¦»æ–‡æœ¬å’Œæ ‡ç­¾
        texts, labels = zip(*training_data)
        texts = list(texts)
        labels = list(labels)

        # åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
        self.label_encoder = {label: i for i, label in enumerate(self.intent_classes)}
        self.id_to_label = {i: label for label, i in self.label_encoder.items()}

        # è½¬æ¢æ ‡ç­¾ä¸ºæ•°å­—
        y = [self.label_encoder[label] for label in labels]

        # æå–ç‰¹å¾
        X = self.extract_features(texts)

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # è®­ç»ƒåˆ†ç±»å™¨
        logger.info("è®­ç»ƒLogisticRegressionåˆ†ç±»å™¨...")
        self.classifier = LogisticRegression(
            random_state=42,
            max_iter=1000,
            multi_class='ovr'  # ä¸€å¯¹å¤šç­–ç•¥
        )

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼ˆLogisticRegressionæœ¬èº«ä¸ç›´æŽ¥æ”¯æŒè¿›åº¦æ¡ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿï¼‰
        logger.info("å¼€å§‹è®­ç»ƒ...")
        self.classifier.fit(X_train, y_train)
        logger.info("è®­ç»ƒå®Œæˆï¼")

        # è¯„ä¼°æ¨¡åž‹
        y_pred = self.classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        logger.info(f"è®­ç»ƒå®Œæˆï¼å‡†ç¡®çŽ‡: {accuracy:.3f}")
        logger.info("åˆ†ç±»æŠ¥å‘Š:")
        logger.info(classification_report(y_test, y_pred, target_names=self.intent_classes))

        return accuracy

    def classify(self, text: str) -> Dict[str, any]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œæ„å›¾åˆ†ç±»

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            Dict: åˆ†ç±»ç»“æžœ
        """
        if not self.classifier:
            raise ValueError("åˆ†ç±»å™¨å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_classifier()")

        # æå–ç‰¹å¾
        embedding = self.extract_features([text])

        # é¢„æµ‹
        intent_id = int(self.classifier.predict(embedding)[0])
        intent = self.id_to_label[intent_id]

        # èŽ·å–é¢„æµ‹æ¦‚çŽ‡
        probabilities = self.classifier.predict_proba(embedding)[0]
        confidence = float(np.max(probabilities))

        # èŽ·å–é…ç½®
        config = self.intent_config[intent]

        result = {
            "intent": intent,
            "confidence": confidence,
            "skip_rag": config["skip_rag"],
            "response": config["response"],
            "probabilities": {
                self.id_to_label[i]: float(prob)
                for i, prob in enumerate(probabilities)
            }
        }

        return result

    def save_model(self, model_dir: str = "./models/intent-classifier"):
        """
        ä¿å­˜æ¨¡åž‹

        Args:
            model_dir: æ¨¡åž‹ä¿å­˜ç›®å½•
        """
        os.makedirs(model_dir, exist_ok=True)

        # ä¿å­˜åˆ†ç±»å™¨
        classifier_path = os.path.join(model_dir, "intent_classifier.pkl")
        joblib.dump(self.classifier, classifier_path)

        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨ï¼ˆå°†id_to_labelçš„æ•´æ•°é”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–ï¼‰
        encoder_path = os.path.join(model_dir, "label_encoder.json")
        with open(encoder_path, 'w', encoding='utf-8') as f:
            json.dump({
                "label_to_id": self.label_encoder,
                "id_to_label": {str(k): v for k, v in self.id_to_label.items()},
                "intent_classes": self.intent_classes,
                "intent_config": self.intent_config
            }, f, ensure_ascii=False, indent=2)

        logger.info(f"æ¨¡åž‹å·²ä¿å­˜åˆ°: {model_dir}")

    def load_model(self, model_dir: str = "./models/intent-classifier"):
        """
        åŠ è½½æ¨¡åž‹

        Args:
            model_dir: æ¨¡åž‹ç›®å½•
        """
        # åŠ è½½åˆ†ç±»å™¨
        classifier_path = os.path.join(model_dir, "intent_classifier.pkl")
        self.classifier = joblib.load(classifier_path)

        # åŠ è½½æ ‡ç­¾ç¼–ç å™¨ï¼ˆå°†JSONä¸­çš„å­—ç¬¦ä¸²é”®è½¬æ¢å›žæ•´æ•°ï¼‰
        encoder_path = os.path.join(model_dir, "label_encoder.json")
        with open(encoder_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.label_encoder = data["label_to_id"]
            self.id_to_label = {int(k): v for k, v in data["id_to_label"].items()}
            self.intent_classes = data["intent_classes"]
            self.intent_config = data["intent_config"]

        logger.info(f"æ¨¡åž‹å·²ä»Ž {model_dir} åŠ è½½å®Œæˆ")


def load_test_data(test_file: str = "test_data.json") -> List[Dict]:
    """
    ä»Žå¤–éƒ¨æ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®

    Args:
        test_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„

    Returns:
        List[Dict]: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    """
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        test_cases = []
        # åˆå¹¶æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        if 'test_cases' in data:
            test_cases.extend(data['test_cases'])
        if 'edge_cases' in data:
            test_cases.extend(data['edge_cases'])
        if 'mixed_language' in data:
            test_cases.extend(data['mixed_language'])

        logger.info(f"ä»Ž {test_file} åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases

    except FileNotFoundError:
        logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ {test_file} ä¸å­˜åœ¨")
        raise
    except Exception as e:
        logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        raise


def main():
    """ä¸»å‡½æ•° - è®­ç»ƒå’Œæµ‹è¯•æ„å›¾åˆ†ç±»å™¨"""
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = IntentClassifier()

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    training_data = classifier.prepare_training_data()

    # è®­ç»ƒåˆ†ç±»å™¨
    accuracy = classifier.train_classifier(training_data)

    # ä¿å­˜æ¨¡åž‹
    classifier.save_model()

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_cases = load_test_data()

    print("\n" + "="*60)
    print("æµ‹è¯•åˆ†ç±»å™¨:")
    print("="*60)

    correct_count = 0
    total_count = len(test_cases)

    for i, test_case in enumerate(test_cases, 1):
        text = test_case['text']
        expected = test_case.get('expected_intent', 'unknown')
        description = test_case.get('description', '')

        result = classifier.classify(text)
        predicted = result['intent']
        is_correct = predicted == expected

        if is_correct:
            correct_count += 1

        status = "âœ…" if is_correct else "âŒ"
        print(f"{status} [{i:2d}/{total_count}] è¾“å…¥: '{text}'")
        print(f"    é¢„æœŸ: {expected:12} | é¢„æµ‹: {predicted:12} | ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"    è¯´æ˜Ž: {description}")
        if result['response']:
            print(f"    å›žå¤: {result['response']}")
        print("-" * 60)

    test_accuracy = correct_count / total_count if total_count > 0 else 0
    print(f"\nðŸ“Š æµ‹è¯•ç»“æžœ:")
    print(f"   æ€»æµ‹è¯•ç”¨ä¾‹: {total_count}")
    print(f"   æ­£ç¡®é¢„æµ‹: {correct_count}")
    print(f"   æµ‹è¯•å‡†ç¡®çŽ‡: {test_accuracy:.3f}")
    print(f"   è®­ç»ƒå‡†ç¡®çŽ‡: {accuracy:.3f}")


if __name__ == "__main__":
    main()
