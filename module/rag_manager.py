"""
RAGç®¡ç†æ¨¡å—
è´Ÿè´£æ–‡æ¡£åŠ è½½ã€åˆ‡åˆ†ã€å‘é‡åº“åˆ›å»ºå’Œç®¡ç†
"""

import os
import glob
import shutil
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from loguru import logger


class RAGManager:
    """RAGçŸ¥è¯†åº“ç®¡ç†å™¨"""

    def __init__(self, config, embeddings):
        """
        åˆå§‹åŒ–RAGç®¡ç†å™¨

        Args:
            config: é…ç½®å¯¹è±¡
            embeddings: åµŒå…¥æ¨¡å‹
        """
        self.config = config
        self.embeddings = embeddings
        self.vector_store = None

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()

    def _ensure_directories(self):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            self.config.DATA_PATH,
            self.config.DATA_NEW_PATH
        ]
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.debug(f"ç›®å½•å·²ç¡®è®¤: {directory}")

    def get_new_documents(self):
        """æ£€æŸ¥å¹¶è·å–æ–°æ–‡æ¡£åˆ—è¡¨ï¼ˆæ”¯æŒPDFå’ŒTXTï¼‰"""
        new_docs = []
        # PDFæ–‡ä»¶
        pdf_pattern = os.path.join(self.config.DATA_NEW_PATH, "*.pdf")
        new_docs.extend(glob.glob(pdf_pattern))
        # TXTæ–‡ä»¶
        txt_pattern = os.path.join(self.config.DATA_NEW_PATH, "*.txt")
        new_docs.extend(glob.glob(txt_pattern))
        return new_docs

    def load_documents_from_path(self, path):
        """
        ä»æŒ‡å®šè·¯å¾„åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒPDFå’ŒTXTï¼‰

        Args:
            path: æ–‡æ¡£ç›®å½•è·¯å¾„

        Returns:
            documents: åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
        """
        documents = []

        # åŠ è½½PDFæ–‡ä»¶
        pdf_pattern = os.path.join(path, "**/*.pdf")
        pdf_files = glob.glob(pdf_pattern, recursive=True)
        for pdf_file in pdf_files:
            logger.info(f"ğŸ“„ åŠ è½½PDF: {os.path.basename(pdf_file)}")
            try:
                loader = PyPDFLoader(pdf_file)
                documents.extend(loader.load())
            except Exception as e:
                logger.error(f"åŠ è½½PDFå¤±è´¥: {pdf_file} - {e}")

        # åŠ è½½TXTæ–‡ä»¶
        txt_pattern = os.path.join(path, "**/*.txt")
        txt_files = glob.glob(txt_pattern, recursive=True)
        for txt_file in txt_files:
            logger.info(f"ğŸ“„ åŠ è½½TXT: {os.path.basename(txt_file)}")
            try:
                loader = TextLoader(txt_file, encoding='utf-8')
                documents.extend(loader.load())
            except Exception as e:
                logger.error(f"åŠ è½½TXTå¤±è´¥: {txt_file} - {e}")

        return documents

    def split_documents(self, documents):
        """
        å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºæ–‡æœ¬å—

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            text_chunks: åˆ‡åˆ†åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP,
            length_function=len,
        )
        text_chunks = text_splitter.split_documents(documents)
        logger.debug(f"æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªå—")
        return text_chunks

    def migrate_processed_documents(self):
        """å°†å·²å¤„ç†çš„æ–‡æ¡£ä»data_newç§»åŠ¨åˆ°dataç›®å½•"""
        new_docs = self.get_new_documents()

        if not new_docs:
            return 0

        migrated_count = 0
        for doc_path in new_docs:
            doc_name = os.path.basename(doc_path)

            try:
                # ç§»åŠ¨åˆ°åŸæ•°æ®ç›®å½•
                dest_path = os.path.join(self.config.DATA_PATH, doc_name)
                shutil.move(doc_path, dest_path)
                logger.debug(f"æ–‡æ¡£å·²ç§»åŠ¨åˆ°dataç›®å½•: {doc_name}")

                migrated_count += 1

            except Exception as e:
                logger.error(f"è¿ç§»æ–‡æ¡£å¤±è´¥ {doc_name}: {str(e)}")

        return migrated_count

    def incremental_load(self):
        """
        å¢é‡åŠ è½½æ–°æ–‡æ¡£åˆ°ç°æœ‰å‘é‡åº“

        Returns:
            new_doc_count: æ–°åŠ è½½çš„æ–‡æ¡£æ•°é‡
        """
        if self.vector_store is None:
            logger.error("å‘é‡åº“æœªåˆå§‹åŒ–")
            return 0

        new_docs = self.get_new_documents()

        if not new_docs:
            logger.debug("æ²¡æœ‰å‘ç°æ–°æ–‡æ¡£")
            return 0

        logger.info(f"ğŸ“„ å‘ç° {len(new_docs)} ä¸ªæ–°æ–‡æ¡£:")
        for doc in new_docs:
            logger.info(f"  - {os.path.basename(doc)}")

        try:
            # åŠ è½½æ–°æ–‡æ¡£
            logger.info("æ­£åœ¨åŠ è½½æ–°æ–‡æ¡£...")
            new_documents = self.load_documents_from_path(self.config.DATA_NEW_PATH)
            logger.info(f"å·²åŠ è½½ {len(new_documents)} ä¸ªæ–°æ–‡æ¡£é¡µé¢")

            # åˆ‡åˆ†æ–‡æ¡£
            logger.info("æ­£åœ¨åˆ‡åˆ†æ–°æ–‡æ¡£...")
            new_chunks = self.split_documents(new_documents)
            logger.info(f"æ–°æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(new_chunks)} ä¸ªå—")

            # åˆ›å»ºæ–°æ–‡æ¡£çš„å‘é‡å¹¶åˆå¹¶åˆ°ç°æœ‰å‘é‡åº“
            logger.info("æ­£åœ¨å°†æ–°æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åº“...")
            new_vector_store = FAISS.from_documents(new_chunks, self.embeddings)

            # åˆå¹¶å‘é‡åº“
            self.vector_store.merge_from(new_vector_store)
            logger.info("å‘é‡åº“åˆå¹¶å®Œæˆ")

            # ä¿å­˜æ›´æ–°åçš„å‘é‡åº“
            self.save()

            # è¿ç§»æ–‡æ¡£
            if self.config.AUTO_MIGRATE_PROCESSED:
                logger.info("æ­£åœ¨è¿ç§»å·²å¤„ç†çš„æ–‡æ¡£...")
                migrated = self.migrate_processed_documents()
                logger.info(f"å·²è¿ç§» {migrated} ä¸ªæ–‡æ¡£")

            return len(new_docs)

        except Exception as e:
            logger.error(f"å¢é‡åŠ è½½å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def create_vector_store(self):
        """åˆ›å»ºåˆå§‹å‘é‡åº“"""
        logger.info("åˆ›å»ºå‘é‡æ•°æ®åº“...")

        # åŠ è½½æ–‡æ¡£
        documents = self.load_documents_from_path(self.config.DATA_PATH)
        logger.info(f"å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # åˆ‡åˆ†æ–‡æ¡£
        text_chunks = self.split_documents(documents)
        logger.info(f"æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªå—")

        # åˆ›å»ºå‘é‡åº“
        logger.info("æ­£åœ¨åˆ›å»ºå‘é‡åº“...")
        self.vector_store = FAISS.from_documents(text_chunks, self.embeddings)

        # æŒä¹…åŒ–ä¿å­˜
        self.save()

        return self.vector_store

    def load_vector_store(self):
        """åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“"""
        if not os.path.exists(self.config.VECTOR_STORE_PATH):
            logger.info("å‘é‡åº“ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„å‘é‡åº“")
            return self.create_vector_store()

        logger.info("åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“...")
        self.vector_store = FAISS.load_local(
            self.config.VECTOR_STORE_PATH,
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        logger.info("å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡æ¡£éœ€è¦å¢é‡åŠ è½½
        if self.config.ENABLE_INCREMENTAL_LOAD:
            logger.debug("æ£€æŸ¥å¢é‡æ–‡æ¡£...")
            try:
                new_doc_count = self.incremental_load()
                if new_doc_count > 0:
                    logger.info(f"ğŸ‰ æˆåŠŸå¢é‡åŠ è½½ {new_doc_count} ä¸ªæ–°æ–‡æ¡£")
            except Exception as e:
                logger.error(f"å¢é‡åŠ è½½å‡ºé”™: {str(e)}")

        return self.vector_store

    def save(self):
        """ä¿å­˜å‘é‡åº“"""
        if self.vector_store is None:
            logger.warning("å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return

        self.vector_store.save_local(self.config.VECTOR_STORE_PATH)
        logger.debug(f"å‘é‡åº“å·²ä¿å­˜åˆ° {self.config.VECTOR_STORE_PATH}")

    def get_retriever(self, search_type="similarity", search_kwargs=None):
        """
        è·å–æ£€ç´¢å™¨

        Args:
            search_type: æ£€ç´¢ç±»å‹
            search_kwargs: æ£€ç´¢å‚æ•°

        Returns:
            retriever: æ£€ç´¢å™¨å¯¹è±¡
        """
        if self.vector_store is None:
            raise ValueError("å‘é‡åº“æœªåˆå§‹åŒ–")

        if search_kwargs is None:
            search_kwargs = {
                "k": self.config.RETRIEVER_K,
                "fetch_k": self.config.RETRIEVER_FETCH_K
            }

        retriever = self.vector_store.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )

        return retriever

