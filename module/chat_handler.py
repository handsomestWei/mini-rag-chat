"""
å¯¹è¯å¤„ç†æ¨¡å—
è´Ÿè´£å¤„ç†ç”¨æˆ·æŸ¥è¯¢å’Œç”Ÿæˆå›ç­”
"""

import time
import logging
import psutil
import os
import json
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler

logger = logging.getLogger(__name__)


class StreamingCallbackHandler(BaseCallbackHandler):
    """æµå¼è¾“å‡ºå›è°ƒå¤„ç†å™¨"""

    def __init__(self):
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """å½“LLMç”Ÿæˆæ–°tokenæ—¶è°ƒç”¨"""
        self.tokens.append(token)

    def get_text(self):
        """è·å–å®Œæ•´æ–‡æœ¬"""
        return "".join(self.tokens)

    def clear(self):
        """æ¸…ç©ºtokens"""
        self.tokens = []


class ChatHandler:
    """å¯¹è¯å¤„ç†å™¨"""

    def __init__(self, chain, query_expander, retriever, config, doc_compressor=None):
        """
        åˆå§‹åŒ–å¯¹è¯å¤„ç†å™¨

        Args:
            chain: LangChainå¯¹è¯é“¾
            query_expander: æŸ¥è¯¢æ‰©å±•å™¨
            retriever: æ£€ç´¢å™¨
            config: é…ç½®å¯¹è±¡
            doc_compressor: æ–‡æ¡£å‹ç¼©å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.chain = chain
        self.query_expander = query_expander
        self.retriever = retriever
        self.config = config
        self.doc_compressor = doc_compressor

        # åˆå§‹åŒ–å®‰å…¨è¿‡æ»¤å™¨
        try:
            from .security_filter import SecurityFilter
            self.security_filter = SecurityFilter(config)
            logger.info("å®‰å…¨è¿‡æ»¤å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"å®‰å…¨è¿‡æ»¤å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.security_filter = None

        # åˆå§‹åŒ–æ„å›¾è¯†åˆ«å™¨
        self.intent_classifier = None
        if getattr(config, 'ENABLE_INTENT_CLASSIFICATION', False):
            try:
                from .intent_classifier import IntentClassifier
                self.intent_classifier = IntentClassifier(config)
                logger.info("æ„å›¾è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            except ImportError as e:
                logger.warning(f"æ„å›¾è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            except Exception as e:
                logger.warning(f"æ„å›¾è¯†åˆ«å™¨åŠ è½½å¤±è´¥: {e}")

    def _get_memory_usage(self):
        """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        mem_mb = mem_info.rss / 1024 / 1024
        mem_percent = process.memory_percent()
        return mem_mb, mem_percent

    def handle_query(self, user_input):
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜

        Returns:
            response: åŒ…å«å›ç­”ã€æ£€ç´¢æ–‡æ¡£ã€æ€§èƒ½ç»Ÿè®¡çš„å­—å…¸
        """
        logger.info("=" * 80)
        logger.info(f"æ”¶åˆ°ç”¨æˆ·é—®é¢˜: {user_input}")
        logger.info("=" * 80)

        # å®‰å…¨éªŒè¯
        if self.security_filter:
            is_valid, processed_input, error_msg = self.security_filter.validate_input(user_input)
            if not is_valid:
                logger.warning(f"è¾“å…¥éªŒè¯å¤±è´¥: {error_msg}")
                return {
                    "answer": error_msg,
                    "source_documents": [],
                    "stats": {
                        "total_time": 0,
                        "retrieval_time": 0,
                        "llm_time": 0,
                        "doc_count": 0,
                        "memory_usage": self._get_memory_usage()[0]
                    }
                }
            user_input = processed_input
            logger.debug(f"è¾“å…¥éªŒè¯é€šè¿‡ï¼Œå¤„ç†åé•¿åº¦: {len(user_input)} å­—ç¬¦")

        # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
        start_time = time.time()
        mem_before, _ = self._get_memory_usage()

        try:
            # ç¬¬1æ­¥ï¼šæŸ¥è¯¢æ‰©å±•
            expanded_query = self.query_expander.expand(user_input)
            logger.debug(f"ğŸ” åŸå§‹æŸ¥è¯¢: {user_input}")
            logger.debug(f"ğŸ” æ‰©å±•æŸ¥è¯¢: {expanded_query}")

            # ç¬¬2æ­¥ï¼šæ–‡æ¡£æ£€ç´¢
            logger.info("æ­¥éª¤1: æ–‡æ¡£æ£€ç´¢")
            retrieval_start = time.time()

            relevant_docs = self.retriever.get_relevant_documents(expanded_query)

            retrieval_time = time.time() - retrieval_start
            logger.info(f"æ£€ç´¢å®Œæˆ (è€—æ—¶: {retrieval_time:.3f}ç§’)")
            logger.info(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")

            # è®°å½•æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆdebugçº§åˆ«ï¼Œä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
            doc_log_lines = ["=" * 80, "ğŸ“„ æ£€ç´¢åˆ°çš„æ–‡æ¡£è¯¦æƒ…:", "=" * 80]
            for i, doc in enumerate(relevant_docs, 1):
                doc_log_lines.append(f"\nã€æ–‡æ¡£ {i}/{len(relevant_docs)}ã€‘")

                # æ‰“å°å…ƒæ•°æ®
                if hasattr(doc, 'metadata') and doc.metadata:
                    doc_log_lines.append(f"ğŸ“‹ å…ƒæ•°æ®: {doc.metadata}")

                # æ‰“å°æ–‡æ¡£å†…å®¹ï¼ˆå®Œæ•´å†…å®¹ï¼‰
                content = doc.page_content
                doc_log_lines.append(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                doc_log_lines.append(f"ğŸ’¬ å®Œæ•´å†…å®¹:\n{content}")
                doc_log_lines.append("-" * 60)

            doc_log_lines.append("=" * 80)
            logger.debug("\n".join(doc_log_lines))

            # ç¬¬3æ­¥ï¼šLLMç”Ÿæˆå›ç­”
            logger.info("æ­¥éª¤2: LLMç”Ÿæˆå›ç­”")
            llm_start = time.time()

            # ä½¿ç”¨åŸå§‹ç”¨æˆ·é—®é¢˜
            chinese_question = user_input

            # Debug: æ‰“å°å‘é€ç»™LLMçš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
            context_log = (
                "=" * 80 + "\n" +
                "ğŸ’­ å‘é€ç»™LLMçš„ä¸Šä¸‹æ–‡:\n" +
                "=" * 80 + "\n" +
                f"ğŸ“‹ é—®é¢˜: {chinese_question}\n" +
                f"ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(relevant_docs)}\n" +
                "=" * 80
            )
            logger.debug(context_log)

            # è°ƒç”¨chainç”Ÿæˆå›ç­”
            result = self.chain({"question": chinese_question, "chat_history": []})

            llm_time = time.time() - llm_start
            logger.info(f"LLMå›ç­”å®Œæˆ (è€—æ—¶: {llm_time:.3f}ç§’)")

            # è·å–å›ç­”
            answer = result["answer"]
            answer_preview = answer[:150].replace('\n', ' ')
            logger.info(f"ç”Ÿæˆçš„å›ç­” (é¢„è§ˆ): {answer_preview}...")

            # Debug: æ‰“å°å®Œæ•´çš„LLMå›ç­”ï¼ˆä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
            answer_log = (
                "=" * 80 + "\n" +
                "ğŸ’¬ LLMç”Ÿæˆçš„å®Œæ•´å›ç­”:\n" +
                "=" * 80 + "\n" +
                f"ğŸ“ å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦\n" +
                f"ğŸ’­ å®Œæ•´å†…å®¹:\n{answer}\n" +
                "=" * 80
            )
            logger.debug(answer_log)

            # è®°å½•æ€»è€—æ—¶å’Œå†…å­˜
            total_time = time.time() - start_time
            mem_after, mem_percent = self._get_memory_usage()

            logger.info("æ€§èƒ½ç»Ÿè®¡:")
            logger.info(f"  - æ£€ç´¢è€—æ—¶: {retrieval_time:.3f}ç§’ ({retrieval_time/total_time*100:.1f}%)")
            logger.info(f"  - LLMè€—æ—¶: {llm_time:.3f}ç§’ ({llm_time/total_time*100:.1f}%)")
            logger.info(f"  - æ€»è€—æ—¶: {total_time:.3f}ç§’")
            logger.debug(f"  - å†…å­˜å˜åŒ–: {mem_before:.2f}MB â†’ {mem_after:.2f}MB")
            logger.info("=" * 80)

            # è¿”å›ç»“æœ
            return {
                "answer": answer,
                "relevant_docs": relevant_docs,
                "stats": {
                    "retrieval_time": retrieval_time,
                    "llm_time": llm_time,
                    "total_time": total_time,
                    "doc_count": len(relevant_docs)
                }
            }

        except Exception as e:
            logger.error("=" * 80)
            logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
            logger.error("=" * 80)
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            raise

    def handle_query_stream(self, user_input):
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜

        Yields:
            dict: æµå¼æ•°æ®å—ï¼ŒåŒ…å« token æˆ–çŠ¶æ€ä¿¡æ¯
        """
        logger.info("=" * 80)
        logger.info(f"æ”¶åˆ°ç”¨æˆ·é—®é¢˜ï¼ˆæµå¼ï¼‰: {user_input}")
        logger.info("=" * 80)

        # å®‰å…¨éªŒè¯
        if self.security_filter:
            is_valid, processed_input, error_msg = self.security_filter.validate_input(user_input)
            if not is_valid:
                logger.warning(f"è¾“å…¥éªŒè¯å¤±è´¥: {error_msg}")
                yield {
                    "type": "error",
                    "message": error_msg
                }
                return
            user_input = processed_input
            logger.debug(f"è¾“å…¥éªŒè¯é€šè¿‡ï¼Œå¤„ç†åé•¿åº¦: {len(user_input)} å­—ç¬¦")

        start_time = time.time()

        try:
            # æ„å›¾è¯†åˆ«
            intent_result = None
            if self.intent_classifier:
                intent_result = self.intent_classifier.classify(user_input)
                logger.info(f"ğŸ¯ æ„å›¾è¯†åˆ«: {intent_result['intent']} "
                           f"(ç½®ä¿¡åº¦: {intent_result['confidence']:.3f}, "
                           f"æ–¹æ³•: {intent_result['method']})")

                # å¦‚æœå¯ä»¥è·³è¿‡RAGï¼Œç›´æ¥è¿”å›é¢„å®šä¹‰å›å¤
                if intent_result['skip_rag'] and intent_result['response']:
                    logger.info(f"è·³è¿‡RAGï¼Œç›´æ¥è¿”å›é¢„å®šä¹‰å›å¤")
                    total_time = time.time() - start_time

                    # å‘é€å®Œæ•´å›å¤
                    yield {
                        "type": "token",
                        "content": intent_result['response']
                    }

                    # å‘é€å®Œæˆä¿¡å·
                    yield {
                        "type": "done",
                        "stats": {
                            "intent": intent_result['intent'],
                            "method": intent_result['method'],
                            "confidence": intent_result['confidence'],
                            "skip_rag": True,
                            "total_time": total_time
                        }
                    }
                    return

            # ç¬¬1æ­¥ï¼šå‘é€æ£€ç´¢çŠ¶æ€
            yield {
                "type": "status",
                "message": self.config.STREAM_STATUS_RETRIEVING,
                "stage": "retrieval"
            }

            # æŸ¥è¯¢æ‰©å±•
            expanded_query = self.query_expander.expand(user_input)
            logger.debug(f"ğŸ” åŸå§‹æŸ¥è¯¢: {user_input}")
            logger.debug(f"ğŸ” æ‰©å±•æŸ¥è¯¢: {expanded_query}")

            # æ–‡æ¡£æ£€ç´¢
            logger.info("æ­¥éª¤1: æ–‡æ¡£æ£€ç´¢")
            retrieval_start = time.time()
            relevant_docs = self.retriever.get_relevant_documents(expanded_query)
            retrieval_time = time.time() - retrieval_start

            logger.info(f"æ£€ç´¢å®Œæˆ (è€—æ—¶: {retrieval_time:.3f}ç§’)")
            logger.info(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")

            # è®°å½•æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£ï¼ˆdebugçº§åˆ«ï¼Œä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
            doc_log_lines = ["=" * 80, "ğŸ“„ æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£:", "=" * 80]
            for i, doc in enumerate(relevant_docs, 1):
                doc_log_lines.append(f"\nã€åŸå§‹æ–‡æ¡£ {i}/{len(relevant_docs)}ã€‘")
                if hasattr(doc, 'metadata') and doc.metadata:
                    doc_log_lines.append(f"ğŸ“‹ å…ƒæ•°æ®: {doc.metadata}")
                doc_log_lines.append(f"ğŸ“ é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
                doc_log_lines.append(f"ğŸ’¬ å†…å®¹:\n{doc.page_content}")
                doc_log_lines.append("-" * 60)
            doc_log_lines.append("=" * 80)
            logger.debug("\n".join(doc_log_lines))

            # æ–‡æ¡£å‹ç¼©
            if self.doc_compressor:
                compression_start = time.time()
                relevant_docs = self.doc_compressor.compress_documents(relevant_docs, user_input)
                compression_time = time.time() - compression_start
                logger.info(f"æ–‡æ¡£å‹ç¼©å®Œæˆ (è€—æ—¶: {compression_time:.3f}ç§’)")

                # è®°å½•å‹ç¼©åçš„æ–‡æ¡£ï¼ˆdebugçº§åˆ«ï¼Œä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
                compressed_log_lines = ["=" * 80, "ğŸ“„ å‹ç¼©åçš„æ–‡æ¡£:", "=" * 80]
                for i, doc in enumerate(relevant_docs, 1):
                    compressed_log_lines.append(f"\nã€å‹ç¼©æ–‡æ¡£ {i}/{len(relevant_docs)}ã€‘")
                    compressed_log_lines.append(f"ğŸ“ é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
                    compressed_log_lines.append(f"ğŸ’¬ å†…å®¹:\n{doc.page_content}")
                    compressed_log_lines.append("-" * 60)
                compressed_log_lines.append("=" * 80)
                logger.debug("\n".join(compressed_log_lines))
            else:
                compression_time = 0
                logger.debug("æ–‡æ¡£å‹ç¼©å™¨æœªå¯ç”¨")

            # ç¬¬2æ­¥ï¼šå‘é€ç”ŸæˆçŠ¶æ€
            yield {
                "type": "status",
                "message": self.config.STREAM_STATUS_GENERATING,
                "stage": "generation"
            }

            # ç¬¬3æ­¥ï¼šå‡†å¤‡ä¸Šä¸‹æ–‡
            logger.info("æ­¥éª¤2: LLMæµå¼ç”Ÿæˆå›ç­”")
            llm_start = time.time()

            # æ‰‹åŠ¨æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå› ä¸ºæµå¼éœ€è¦ç›´æ¥è°ƒç”¨LLMï¼‰
            context = "\n\n".join([doc.page_content for doc in relevant_docs])

            # æ„å»ºå®Œæ•´æç¤ºè¯
            from langchain.prompts import PromptTemplate
            prompt_template = PromptTemplate(
                template=self.config.SYSTEM_PROMPT + "\n\n" + self.config.USER_QUESTION_TEMPLATE,
                input_variables=["context", "question"]
            )

            full_prompt = prompt_template.format(
                context=context,
                question=user_input
            )

            # Debug: æ‰“å°å®Œæ•´çš„æç¤ºè¯ï¼ˆä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
            prompt_log = (
                "=" * 80 + "\n" +
                "ğŸ’­ å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯ï¼ˆæµå¼æ¨¡å¼ï¼‰:\n" +
                "=" * 80 + "\n" +
                full_prompt + "\n" +
                "=" * 80
            )
            logger.debug(prompt_log)

            # ä½¿ç”¨ LLM çš„æµå¼æ¥å£
            # éœ€è¦è®¿é—®åŸå§‹ LLM å¯¹è±¡
            llm = self.chain.combine_docs_chain.llm_chain.llm

            # æµå¼ç”Ÿæˆ
            full_answer = ""
            for chunk in llm.stream(full_prompt):
                if chunk:
                    full_answer += chunk
                    yield {
                        "type": "token",
                        "content": chunk
                    }

            llm_time = time.time() - llm_start
            total_time = time.time() - start_time

            logger.info(f"LLMæµå¼å›ç­”å®Œæˆ (è€—æ—¶: {llm_time:.3f}ç§’)")
            logger.info(f"æ€»è€—æ—¶: {total_time:.3f}ç§’")

            # Debug: æ‰“å°æµå¼ç”Ÿæˆçš„å®Œæ•´å›ç­”ï¼ˆä½¿ç”¨å•æ¡æ—¥å¿—é¿å…é‡å¤ï¼‰
            answer_log = (
                "=" * 80 + "\n" +
                "ğŸ’¬ LLMæµå¼ç”Ÿæˆçš„å®Œæ•´å›ç­”:\n" +
                "=" * 80 + "\n" +
                f"ğŸ“ å›ç­”é•¿åº¦: {len(full_answer)} å­—ç¬¦\n" +
                f"ğŸ’­ å®Œæ•´å†…å®¹:\n{full_answer}\n" +
                "=" * 80
            )
            logger.debug(answer_log)

            # ç¬¬4æ­¥ï¼šå‘é€å®ŒæˆçŠ¶æ€
            stats = {
                "retrieval_time": retrieval_time,
                "llm_time": llm_time,
                "total_time": total_time,
                "doc_count": len(relevant_docs),
                "skip_rag": False
            }

            # æ·»åŠ å‹ç¼©æ—¶é—´ä¿¡æ¯
            if self.doc_compressor and compression_time > 0:
                stats["compression_time"] = compression_time

            # æ·»åŠ æ„å›¾è¯†åˆ«ä¿¡æ¯
            if intent_result:
                stats.update({
                    "intent": intent_result['intent'],
                    "intent_method": intent_result['method'],
                    "intent_confidence": intent_result['confidence']
                })

            yield {
                "type": "done",
                "stats": stats
            }

        except Exception as e:
            logger.error("=" * 80)
            logger.error(f"æµå¼å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
            logger.error("=" * 80)
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)

            yield {
                "type": "error",
                "message": f"å¤„ç†å‡ºé”™: {str(e)}"
            }

